{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab37800",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hand gestures detection using Tensorflow ssd_mobilenet_v2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66894944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import uuid\n",
    "import time\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae4cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 hand gestures to be processed finally\n",
    "\n",
    "labels = ['good', 'bad', 'perfect', 'stop', 'zero']\n",
    "number_imgs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebe11c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "\n",
    "cd = \"/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project\"\n",
    "images_folder = os.path.join(cd, \"images\")\n",
    "\n",
    "# folders to contain the source images\n",
    "if not os.path.exists(images_folder):\n",
    "    os.mkdir(images_folder)\n",
    "\n",
    "for label in labels:\n",
    "    path = os.path.join(images_folder, label)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f9110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capturing photos through webcam\n",
    "\n",
    "photos_per_label = 20\n",
    "\n",
    "for label in labels:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    print(f'Now capturing --> {label}')\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    for i in range(photos_per_label):\n",
    "        print(f'Image {i}/{photos_per_label}')\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        filename = os.path.join(images_folder,label,label+'.'+f'{str(uuid.uuid1())}.jpg') \n",
    "        cv2.imwrite(filename, frame)\n",
    "        cv2.imshow('frame', frame)\n",
    "        time.sleep(3)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelimg from github\n",
    "\n",
    "lblimg_folder = os.path.join(cd, \"labelimg\")\n",
    "\n",
    "if not os.path.exists(lblimg_folder):\n",
    "    os.mkdir(lblimg_folder)\n",
    "    !git clone https://github.com/tzutalin/labelImg '{lblimg_folder}'\n",
    "\n",
    "\n",
    "!cd '{lblimg_folder}' && pyrcc5 -o libs/resources.py resources.qrc        \n",
    "# subprocess.run([\"python3\", temp])\n",
    "\n",
    "!cd '{lblimg_folder}' && python3 labelImg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e67b18",
   "metadata": {},
   "source": [
    "### Second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9851cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory management\n",
    "\n",
    "cd = \"/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project\"\n",
    "model_name = \"my_model\"\n",
    "pretrained_model_name = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "pretrained_model_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "recordscript = 'generate_tfrecord.py'\n",
    "labelmap = 'label_map.pbtxt'\n",
    "\n",
    "paths = {\n",
    "    'workspace': os.path.join(cd, 'workspace'),\n",
    "    'scripts': os.path.join(cd,'scripts'),\n",
    "    'apimodel': os.path.join(cd,'models'),\n",
    "    'annotations': os.path.join(cd, 'workspace','annotations'),\n",
    "    'images': os.path.join(cd, 'workspace','images'),\n",
    "    'model': os.path.join(cd, 'workspace','models'),\n",
    "    'pretrained_model': os.path.join(cd, 'workspace','pretrainedmodels'),\n",
    "    'checkpoint': os.path.join(cd, 'workspace','models',model_name), \n",
    "    'output': os.path.join(cd, 'workspace','models',model_name, 'export'), \n",
    "    'tfjs':os.path.join(cd, 'workspace','models',model_name, 'tfjsexport'), \n",
    "    'tflite':os.path.join(cd, 'workspace','models',model_name, 'tfliteexport'), \n",
    "    'protoc':os.path.join(cd,'protoc')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'pipeline_config':os.path.join(cd, 'workspace','models', model_name, 'pipeline.config'),\n",
    "    'recordscript': os.path.join(paths['scripts'], recordscript), \n",
    "    'labelmap': os.path.join(paths['annotations'], labelmap)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9453fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5551e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading pretrained models\n",
    "\n",
    "if not os.path.exists(os.path.join(paths['apimodel'], 'research', 'object_detection')):\n",
    "    !git clone https://github.com/tensorflow/models \"{paths['apimodel']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "612f8326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/models/research\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting avro-python3 (from object-detection==0.1)\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting apache-beam (from object-detection==0.1)\n",
      "  Downloading apache_beam-2.51.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (9.4.0)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (4.9.3)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (3.8.0)\n",
      "Requirement already satisfied: Cython in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (3.0.4)\n",
      "Requirement already satisfied: contextlib2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (21.6.0)\n",
      "Collecting tf-slim (from object-detection==0.1)\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (1.16.0)\n",
      "Collecting pycocotools (from object-detection==0.1)\n",
      "  Downloading pycocotools-2.0.7-cp311-cp311-macosx_10_9_universal2.whl.metadata (1.1 kB)\n",
      "Collecting lvis (from object-detection==0.1)\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (1.11.3)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (2.1.1)\n",
      "Collecting tf-models-official>=2.5.1 (from object-detection==0.1)\n",
      "  Downloading tf_models_official-2.14.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting tensorflow_io (from object-detection==0.1)\n",
      "  Downloading tensorflow_io-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: keras in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from object-detection==0.1) (2.14.0)\n",
      "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
      "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
      "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting regex (from sacrebleu<=2.2.0->object-detection==0.1)\n",
      "  Downloading regex-2023.10.3-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tabulate>=0.8.9 (from sacrebleu<=2.2.0->object-detection==0.1)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.26.0)\n",
      "Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting gin-config (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-api-python-client>=1.6.7 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading google_api_python_client-2.104.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting immutabledict (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading immutabledict-3.0.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting kaggle>=1.3.9 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting oauth2client (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python-headless (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.9.4)\n",
      "Collecting py-cpuinfo>=3.3.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (6.0)\n",
      "Collecting sentencepiece (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading sentencepiece-0.1.99-cp311-cp311-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting seqeval (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-datasets (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting tensorflow-hub>=0.6.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
      "Collecting tensorflow-text~=2.14.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tensorflow_text-2.14.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: tensorflow~=2.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->object-detection==0.1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->object-detection==0.1) (2023.3)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tf-slim->object-detection==0.1) (2.0.0)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting orjson<4,>=3.9.7 (from apache-beam->object-detection==0.1)\n",
      "  Downloading orjson-3.9.9-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cloudpickle~=2.2.1 (from apache-beam->object-detection==0.1)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
      "  Downloading fastavro-1.8.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from apache-beam->object-detection==0.1) (1.59.0)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting httplib2<0.23.0,>=0.8 (from apache-beam->object-detection==0.1)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting js2py<1,>=0.74 (from apache-beam->object-detection==0.1)\n",
      "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hCollecting numpy>=1.17 (from sacrebleu<=2.2.0->object-detection==0.1)\n",
      "  Downloading numpy-1.24.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting objsize<0.7.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
      "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from apache-beam->object-detection==0.1) (23.0)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
      "  Downloading pymongo-4.5.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (22 kB)\n",
      "Collecting proto-plus<2,>=1.7.1 (from apache-beam->object-detection==0.1)\n",
      "  Downloading proto_plus-1.22.3-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.0,!=4.24.1,!=4.24.2,<4.25.0,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from apache-beam->object-detection==0.1) (4.24.4)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam->object-detection==0.1)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from apache-beam->object-detection==0.1) (4.8.0)\n",
      "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
      "  Downloading zstandard-0.21.0-cp311-cp311-macosx_10_9_x86_64.whl (473 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow<12.0.0,>=3.0.0 (from apache-beam->object-detection==0.1)\n",
      "  Downloading pyarrow-11.0.0-cp311-cp311-macosx_10_14_x86_64.whl (24.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.4/24.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from lvis->object-detection==0.1) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from lvis->object-detection==0.1) (1.4.5)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from lvis->object-detection==0.1) (4.8.1.78)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->object-detection==0.1) (1.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib->object-detection==0.1) (4.42.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.34.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow_io->object-detection==0.1) (0.34.0)\n",
      "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.23.3)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tzlocal>=1.2 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam->object-detection==0.1)\n",
      "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2023.7.22)\n",
      "Collecting tqdm (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-slugify (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: urllib3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.0.5)\n",
      "Requirement already satisfied: bleach in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.0.0)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
      "  Downloading dnspython-2.4.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (65.5.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.14.0)\n",
      "Collecting absl-py>=0.2.2 (from tf-slim->object-detection==0.1)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree~=0.1.1 (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-macosx_10_9_x86_64.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.3.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
      "Collecting scikit-learn>=0.21.3 (from seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading scikit_learn-1.3.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting array-record (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading array_record-0.4.1-py310-none-any.whl.metadata (503 bytes)\n",
      "Collecting click (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting etils>=0.9.0 (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading etils-1.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting promise (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-metadata (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting toml (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.41.2)\n",
      "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting zipp (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.0.0)\n",
      "Requirement already satisfied: webencodings in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.0,!=4.24.1,!=4.24.2,<4.25.0,>=3.20.3 (from apache-beam->object-detection==0.1)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow~=2.14.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
      "Downloading tf_models_official-2.14.2-py2.py3-none-any.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading apache_beam-2.51.0-cp311-cp311-macosx_10_9_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pycocotools-2.0.7-cp311-cp311-macosx_10_9_universal2.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl (25.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.3/25.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.8.4-cp311-cp311-macosx_10_9_universal2.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading google_api_python_client-2.104.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.24.4-cp311-cp311-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.9-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymongo-4.5.0-cp311-cp311-macosx_10_9_universal2.whl (529 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.4/529.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_text-2.14.0-cp311-cp311-macosx_10_9_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
      "Downloading opencv_python_headless-4.8.1.78-cp37-abi3-macosx_10_16_x86_64.whl (54.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.5.1-py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading scikit_learn-1.3.1-cp311-cp311-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading array_record-0.4.1-py310-none-any.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, kaggle, seqeval, pyjsparser, docopt, promise\n",
      "  Building wheel for object-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1656034 sha256=78ecb76a9a8b30548d5e9409da0ac3962d0db10142f4f32e9b4d68d7dd9de0e9\n",
      "  Stored in directory: /private/var/folders/_9/wn4fwgv90xq2sj6mzd3sdw1c0000gn/T/pip-ephem-wheel-cache-ioeby75w/wheels/9a/a0/8b/36db44c27271a622fa09c39717a1741386a46a316dc0fb0c65\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43991 sha256=ab86652fd8c4c494a6fc7c482ddd789a4548b9a7d9cadfcc8793aeb0aa99853c\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/31/be/50/1145d9510eb4440893fc0ec676ef9464a05e0f7492a76fbb2c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-py3-none-any.whl size=18833 sha256=d640e19f4871afee610e442d3696ff95270cb5691e1cdb6780fa5ef2dda47aae\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=678ffc91fe2bacac76eb1bfbcde555d27d4e3cad5d57029bb9c75120e3f89515\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/01/60/80/1622338bcecce31a5664ef01c203cc5a7b09f59588d9c07376\n",
      "  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=a649080510c4491ab148708ba353aa7c92283a74ac9fdfdcf8bbb5836b80fc07\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/b9/1d/dc/eb0833be25464c359903d356c4204721c6a672c26ff164cdc3\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110683 sha256=d2d7063b42074a831cf99d01c5b2c7f7ebdeec363ab19ee9cb038c6d3a235566\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/6a/2b/d0/457dd27de499e9423caf738e743c4a3f82886ee6b19f89d5b7\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=398115c939e193161f45da75318065b1c3f9d51ed4f004c33813d0565486209d\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "  Building wheel for pyjsparser (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25982 sha256=f7672bc32424672be4f676087f0c44f60cb0daf5628e4fedaf56e164611815f2\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/a5/9a/30/1003e89ab4555b81840ca46d361bf184f1e6ad880cae3b62a9\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=4da195f3d30d8781c03e4e48085e87b9b862332dbe88aceb05d6806197dca458\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21484 sha256=58212b65bdbf720c212b440d95b82bc1fc481c15479532c2a127dc0e43098492\n",
      "  Stored in directory: /Users/nazmussakib/Library/Caches/pip/wheels/90/74/b1/9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
      "Successfully built object-detection avro-python3 crcmod dill hdfs kaggle seqeval pyjsparser docopt promise\n",
      "Installing collected packages: text-unidecode, sentencepiece, pyjsparser, py-cpuinfo, gin-config, docopt, dm-tree, crcmod, zstandard, zipp, uritemplate, tzlocal, tqdm, toml, threadpoolctl, tensorflow_io, tabulate, regex, python-slugify, pyparsing, protobuf, promise, portalocker, orjson, objsize, numpy, joblib, importlib_resources, immutabledict, fsspec, fasteners, fastavro, etils, dnspython, dill, colorama, cloudpickle, click, avro-python3, absl-py, tf-slim, tensorflow-model-optimization, tensorflow-hub, sacrebleu, pymongo, pydot, pyarrow, proto-plus, opencv-python-headless, kaggle, js2py, httplib2, hdfs, googleapis-common-protos, tensorflow-metadata, scikit-learn, oauth2client, google-auth-httplib2, google-api-core, apache-beam, seqeval, pycocotools, lvis, google-api-python-client, array-record, tensorflow-text, tensorflow-datasets, tf-models-official, object-detection\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.1\n",
      "    Uninstalling pyparsing-3.1.1:\n",
      "      Successfully uninstalled pyparsing-3.1.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.4\n",
      "    Uninstalling protobuf-4.24.4:\n",
      "      Successfully uninstalled protobuf-4.24.4\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.0.0\n",
      "    Uninstalling absl-py-2.0.0:\n",
      "      Successfully uninstalled absl-py-2.0.0\n",
      "  Attempting uninstall: object-detection\n",
      "    Found existing installation: object-detection 0.0.3\n",
      "    Uninstalling object-detection-0.0.3:\n",
      "      Successfully uninstalled object-detection-0.0.3\n",
      "Successfully installed absl-py-1.4.0 apache-beam-2.51.0 array-record-0.4.1 avro-python3-1.10.2 click-8.1.7 cloudpickle-2.2.1 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dm-tree-0.1.8 dnspython-2.4.2 docopt-0.6.2 etils-1.5.1 fastavro-1.8.4 fasteners-0.19 fsspec-2023.10.0 gin-config-0.5.0 google-api-core-2.12.0 google-api-python-client-2.104.0 google-auth-httplib2-0.1.1 googleapis-common-protos-1.61.0 hdfs-2.7.3 httplib2-0.22.0 immutabledict-3.0.0 importlib_resources-6.1.0 joblib-1.3.2 js2py-0.74 kaggle-1.5.16 lvis-0.5.3 numpy-1.24.4 oauth2client-4.1.3 object-detection-0.1 objsize-0.6.1 opencv-python-headless-4.8.1.78 orjson-3.9.9 portalocker-2.8.2 promise-2.3 proto-plus-1.22.3 protobuf-3.20.3 py-cpuinfo-9.0.0 pyarrow-11.0.0 pycocotools-2.0.7 pydot-1.4.2 pyjsparser-2.7.1 pymongo-4.5.0 pyparsing-2.4.7 python-slugify-8.0.1 regex-2023.10.3 sacrebleu-2.2.0 scikit-learn-1.3.1 sentencepiece-0.1.99 seqeval-1.2.2 tabulate-0.9.0 tensorflow-datasets-4.9.3 tensorflow-hub-0.15.0 tensorflow-metadata-1.14.0 tensorflow-model-optimization-0.7.5 tensorflow-text-2.14.0 tensorflow_io-0.34.0 text-unidecode-1.3 tf-models-official-2.14.2 tf-slim-1.1.0 threadpoolctl-3.2.0 toml-0.10.2 tqdm-4.66.1 tzlocal-5.2 uritemplate-4.1.1 zipp-3.17.0 zstandard-0.21.0\n"
     ]
    }
   ],
   "source": [
    "# Install TF obejct detection\n",
    "\n",
    "temp = os.path.join(cd, \"models/research\")\n",
    "\n",
    "!cd '{temp}' && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py .  && python3 -m pip install . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fb2752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:57:57.334195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Running tests under Python 3.11.1: /Library/Frameworks/Python.framework/Versions/3.11/bin/python3\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "W1023 16:58:31.488821 4640491008 batch_normalization.py:1531] `tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/model_builder.py:1112: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(('Building experimental DeepMAC meta-arch.'\n",
      "W1023 16:58:31.931940 4640491008 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.72s\n",
      "I1023 16:58:33.001026 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.72s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 2.34s\n",
      "I1023 16:58:35.345165 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 2.34s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.6s\n",
      "I1023 16:58:35.950206 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.6s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.48s\n",
      "I1023 16:58:36.429486 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.48s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 5.36s\n",
      "I1023 16:58:41.788337 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 5.36s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "I1023 16:58:41.793361 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
      "I1023 16:58:41.831603 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.04s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
      "I1023 16:58:41.857464 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
      "I1023 16:58:41.885334 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.03s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.19s\n",
      "I1023 16:58:42.074499 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.17s\n",
      "I1023 16:58:42.244749 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.17s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.19s\n",
      "I1023 16:58:42.431482 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.19s\n",
      "I1023 16:58:42.621583 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.57s\n",
      "I1023 16:58:43.193028 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.57s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.49s\n",
      "I1023 16:58:43.689135 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.49s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "I1023 16:58:44.809262 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
      "I1023 16:58:44.809414 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 64\n",
      "I1023 16:58:44.809511 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 3\n",
      "I1023 16:58:44.816093 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1023 16:58:45.280527 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I1023 16:58:45.280669 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:45.922070 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:45.922308 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:46.771460 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:46.771625 4640491008 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I1023 16:58:47.359140 4640491008 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I1023 16:58:47.359310 4640491008 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I1023 16:58:47.997551 4640491008 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I1023 16:58:47.997753 4640491008 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I1023 16:58:48.772431 4640491008 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I1023 16:58:48.772613 4640491008 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I1023 16:58:49.663316 4640491008 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I1023 16:58:49.663520 4640491008 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I1023 16:58:49.848993 4640491008 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I1023 16:58:49.976396 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:58:50.077085 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
      "I1023 16:58:50.077231 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 88\n",
      "I1023 16:58:50.077312 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 4\n",
      "I1023 16:58:50.080095 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I1023 16:58:50.190678 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I1023 16:58:50.190871 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:50.588711 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:50.588852 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:51.194438 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:51.194582 4640491008 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I1023 16:58:51.754662 4640491008 efficientnet_model.py:143] round_filter input=40 output=40\n",
      "I1023 16:58:51.754802 4640491008 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I1023 16:58:52.455352 4640491008 efficientnet_model.py:143] round_filter input=80 output=80\n",
      "I1023 16:58:52.455497 4640491008 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I1023 16:58:53.181542 4640491008 efficientnet_model.py:143] round_filter input=112 output=112\n",
      "I1023 16:58:53.181712 4640491008 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I1023 16:58:55.562654 4640491008 efficientnet_model.py:143] round_filter input=192 output=192\n",
      "I1023 16:58:55.562865 4640491008 efficientnet_model.py:143] round_filter input=320 output=320\n",
      "I1023 16:58:56.362308 4640491008 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
      "I1023 16:58:56.490650 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:58:56.625160 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
      "I1023 16:58:56.625303 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 112\n",
      "I1023 16:58:56.625385 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 5\n",
      "I1023 16:58:56.627573 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I1023 16:58:56.680444 4640491008 efficientnet_model.py:143] round_filter input=32 output=32\n",
      "I1023 16:58:56.680598 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:57.114470 4640491008 efficientnet_model.py:143] round_filter input=16 output=16\n",
      "I1023 16:58:57.114609 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:57.745579 4640491008 efficientnet_model.py:143] round_filter input=24 output=24\n",
      "I1023 16:58:57.745734 4640491008 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I1023 16:58:58.412197 4640491008 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I1023 16:58:58.412333 4640491008 efficientnet_model.py:143] round_filter input=80 output=88\n",
      "I1023 16:58:59.280616 4640491008 efficientnet_model.py:143] round_filter input=80 output=88\n",
      "I1023 16:58:59.280752 4640491008 efficientnet_model.py:143] round_filter input=112 output=120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1023 16:59:00.282938 4640491008 efficientnet_model.py:143] round_filter input=112 output=120\n",
      "I1023 16:59:00.283098 4640491008 efficientnet_model.py:143] round_filter input=192 output=208\n",
      "I1023 16:59:01.570639 4640491008 efficientnet_model.py:143] round_filter input=192 output=208\n",
      "I1023 16:59:01.570878 4640491008 efficientnet_model.py:143] round_filter input=320 output=352\n",
      "I1023 16:59:02.018357 4640491008 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
      "I1023 16:59:02.127152 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:59:02.233358 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
      "I1023 16:59:02.233514 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 160\n",
      "I1023 16:59:02.233608 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 6\n",
      "I1023 16:59:02.235900 4640491008 efficientnet_model.py:143] round_filter input=32 output=40\n",
      "I1023 16:59:02.277930 4640491008 efficientnet_model.py:143] round_filter input=32 output=40\n",
      "I1023 16:59:02.278100 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:02.575423 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:02.575572 4640491008 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I1023 16:59:03.325786 4640491008 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I1023 16:59:03.325946 4640491008 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I1023 16:59:04.329581 4640491008 efficientnet_model.py:143] round_filter input=40 output=48\n",
      "I1023 16:59:04.329730 4640491008 efficientnet_model.py:143] round_filter input=80 output=96\n",
      "I1023 16:59:05.640046 4640491008 efficientnet_model.py:143] round_filter input=80 output=96\n",
      "I1023 16:59:05.640390 4640491008 efficientnet_model.py:143] round_filter input=112 output=136\n",
      "I1023 16:59:06.623965 4640491008 efficientnet_model.py:143] round_filter input=112 output=136\n",
      "I1023 16:59:06.624105 4640491008 efficientnet_model.py:143] round_filter input=192 output=232\n",
      "I1023 16:59:08.547122 4640491008 efficientnet_model.py:143] round_filter input=192 output=232\n",
      "I1023 16:59:08.547337 4640491008 efficientnet_model.py:143] round_filter input=320 output=384\n",
      "I1023 16:59:09.619693 4640491008 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
      "I1023 16:59:09.772826 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:59:09.895138 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
      "I1023 16:59:09.895256 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 224\n",
      "I1023 16:59:09.895315 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
      "I1023 16:59:09.897366 4640491008 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I1023 16:59:09.945292 4640491008 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I1023 16:59:09.945415 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:10.265154 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:10.265273 4640491008 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I1023 16:59:11.617841 4640491008 efficientnet_model.py:143] round_filter input=24 output=32\n",
      "I1023 16:59:11.618011 4640491008 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I1023 16:59:13.605841 4640491008 efficientnet_model.py:143] round_filter input=40 output=56\n",
      "I1023 16:59:13.606060 4640491008 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I1023 16:59:16.112245 4640491008 efficientnet_model.py:143] round_filter input=80 output=112\n",
      "I1023 16:59:16.112648 4640491008 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I1023 16:59:17.723255 4640491008 efficientnet_model.py:143] round_filter input=112 output=160\n",
      "I1023 16:59:17.723405 4640491008 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I1023 16:59:19.764082 4640491008 efficientnet_model.py:143] round_filter input=192 output=272\n",
      "I1023 16:59:19.764271 4640491008 efficientnet_model.py:143] round_filter input=320 output=448\n",
      "I1023 16:59:20.248574 4640491008 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
      "I1023 16:59:20.369570 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1023 16:59:20.509403 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
      "I1023 16:59:20.509660 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 288\n",
      "I1023 16:59:20.509817 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 7\n",
      "I1023 16:59:20.513966 4640491008 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I1023 16:59:20.548094 4640491008 efficientnet_model.py:143] round_filter input=32 output=48\n",
      "I1023 16:59:20.548265 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:21.058006 4640491008 efficientnet_model.py:143] round_filter input=16 output=24\n",
      "I1023 16:59:21.058196 4640491008 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I1023 16:59:22.344664 4640491008 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I1023 16:59:22.344923 4640491008 efficientnet_model.py:143] round_filter input=40 output=64\n",
      "I1023 16:59:23.468656 4640491008 efficientnet_model.py:143] round_filter input=40 output=64\n",
      "I1023 16:59:23.468807 4640491008 efficientnet_model.py:143] round_filter input=80 output=128\n",
      "I1023 16:59:24.902070 4640491008 efficientnet_model.py:143] round_filter input=80 output=128\n",
      "I1023 16:59:24.902354 4640491008 efficientnet_model.py:143] round_filter input=112 output=176\n",
      "I1023 16:59:26.464854 4640491008 efficientnet_model.py:143] round_filter input=112 output=176\n",
      "I1023 16:59:26.464995 4640491008 efficientnet_model.py:143] round_filter input=192 output=304\n",
      "I1023 16:59:28.997928 4640491008 efficientnet_model.py:143] round_filter input=192 output=304\n",
      "I1023 16:59:28.998070 4640491008 efficientnet_model.py:143] round_filter input=320 output=512\n",
      "I1023 16:59:30.013760 4640491008 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
      "I1023 16:59:30.168064 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:59:30.312729 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
      "I1023 16:59:30.312922 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
      "I1023 16:59:30.313048 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
      "I1023 16:59:30.316298 4640491008 efficientnet_model.py:143] round_filter input=32 output=56\n",
      "I1023 16:59:30.356126 4640491008 efficientnet_model.py:143] round_filter input=32 output=56\n",
      "I1023 16:59:30.356295 4640491008 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I1023 16:59:30.845665 4640491008 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I1023 16:59:30.845789 4640491008 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I1023 16:59:32.064860 4640491008 efficientnet_model.py:143] round_filter input=24 output=40\n",
      "I1023 16:59:32.065185 4640491008 efficientnet_model.py:143] round_filter input=40 output=72\n",
      "I1023 16:59:33.281321 4640491008 efficientnet_model.py:143] round_filter input=40 output=72\n",
      "I1023 16:59:33.281566 4640491008 efficientnet_model.py:143] round_filter input=80 output=144\n",
      "I1023 16:59:34.966713 4640491008 efficientnet_model.py:143] round_filter input=80 output=144\n",
      "I1023 16:59:34.966846 4640491008 efficientnet_model.py:143] round_filter input=112 output=200\n",
      "I1023 16:59:36.683788 4640491008 efficientnet_model.py:143] round_filter input=112 output=200\n",
      "I1023 16:59:36.683923 4640491008 efficientnet_model.py:143] round_filter input=192 output=344\n",
      "I1023 16:59:39.492994 4640491008 efficientnet_model.py:143] round_filter input=192 output=344\n",
      "I1023 16:59:39.493122 4640491008 efficientnet_model.py:143] round_filter input=320 output=576\n",
      "I1023 16:59:40.321953 4640491008 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
      "I1023 16:59:40.496184 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "I1023 16:59:40.894634 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:161] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
      "I1023 16:59:40.894829 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:163] EfficientDet BiFPN num filters: 384\n",
      "I1023 16:59:40.894943 4640491008 ssd_efficientnet_bifpn_feature_extractor.py:164] EfficientDet BiFPN num iterations: 8\n",
      "I1023 16:59:40.897434 4640491008 efficientnet_model.py:143] round_filter input=32 output=64\n",
      "I1023 16:59:40.953439 4640491008 efficientnet_model.py:143] round_filter input=32 output=64\n",
      "I1023 16:59:40.953650 4640491008 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I1023 16:59:41.596125 4640491008 efficientnet_model.py:143] round_filter input=16 output=32\n",
      "I1023 16:59:41.596279 4640491008 efficientnet_model.py:143] round_filter input=24 output=48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I1023 16:59:42.966599 4640491008 efficientnet_model.py:143] round_filter input=24 output=48\n",
      "I1023 16:59:42.966742 4640491008 efficientnet_model.py:143] round_filter input=40 output=80\n",
      "I1023 16:59:44.404493 4640491008 efficientnet_model.py:143] round_filter input=40 output=80\n",
      "I1023 16:59:44.404646 4640491008 efficientnet_model.py:143] round_filter input=80 output=160\n",
      "I1023 16:59:46.439630 4640491008 efficientnet_model.py:143] round_filter input=80 output=160\n",
      "I1023 16:59:46.439813 4640491008 efficientnet_model.py:143] round_filter input=112 output=224\n",
      "I1023 16:59:49.195858 4640491008 efficientnet_model.py:143] round_filter input=112 output=224\n",
      "I1023 16:59:49.195994 4640491008 efficientnet_model.py:143] round_filter input=192 output=384\n",
      "I1023 16:59:52.297540 4640491008 efficientnet_model.py:143] round_filter input=192 output=384\n",
      "I1023 16:59:52.297677 4640491008 efficientnet_model.py:143] round_filter input=320 output=640\n",
      "I1023 16:59:53.535075 4640491008 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
      "I1023 16:59:53.679078 4640491008 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 70.19s\n",
      "I1023 16:59:53.882038 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 70.19s\n",
      "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "I1023 16:59:53.919199 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "I1023 16:59:53.920778 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "I1023 16:59:53.921189 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
      "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "I1023 16:59:53.922563 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
      "[ RUN      ] ModelBuilderTF2Test.test_session\n",
      "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "I1023 16:59:53.923839 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "I1023 16:59:53.924232 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
      "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "I1023 16:59:53.925591 4640491008 test_util.py:2472] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
      "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
      "----------------------------------------------------------------------\n",
      "Ran 24 tests in 82.640s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "VERIFICATION_SCRIPT = os.path.join(paths['apimodel'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "# Verify Installation\n",
    "!python3 '{VERIFICATION_SCRIPT}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb374ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 4.24.4\n",
      "Uninstalling protobuf-4.24.4:\n",
      "  Successfully uninstalled protobuf-4.24.4\n",
      "\u001b[33mWARNING: Skipping matplotlib as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf\n",
      "  Using cached protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (540 bytes)\n",
      "Using cached protobuf-4.24.4-cp37-abi3-macosx_10_9_universal2.whl (409 kB)\n",
      "Installing collected packages: protobuf\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-models-official 2.14.2 requires matplotlib, which is not installed.\n",
      "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.24.4\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.0-cp311-cp311-macosx_10_12_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.0-cp311-cp311-macosx_10_12_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: matplotlib\n",
      "Successfully installed matplotlib-3.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --upgrade\n",
    "!pip uninstall protobuf matplotlib -y\n",
    "!pip install protobuf\n",
    "!pip install matplotlib\n",
    "# !pip3 install protobuf matplotlib==3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7994dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-23 17:19:54--  http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
      "Resolving download.tensorflow.org (download.tensorflow.org)... 142.250.196.27, 142.250.196.59, 142.250.76.91, ...\n",
      "Connecting to download.tensorflow.org (download.tensorflow.org)|142.250.196.27|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20515344 (20M) [application/x-tar]\n",
      "Saving to: ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz.1’\n",
      "\n",
      "ssd_mobilenet_v2_fp 100%[===================>]  19.56M  3.58MB/s    in 5.4s    \n",
      "\n",
      "2023-10-23 17:20:00 (3.65 MB/s) - ‘ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz.1’ saved [20515344/20515344]\n",
      "\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
      "x ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!wget {pretrained_model_url}\n",
    "!mv \"{pretrained_model_name+'.tar.gz'}\" \"{paths['pretrained_model']}\"\n",
    "!cd \"{paths['pretrained_model']}\" && tar -zxvf \"{pretrained_model_name+'.tar.gz'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0478601a",
   "metadata": {},
   "source": [
    "### main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d56fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'good', 'id':1}, {'name':'bad', 'id':2}, {'name':'perfect', 'id':3}, {'name':'stop', 'id':4}, {'name':'zero', 'id':5}]\n",
    "\n",
    "with open(files['labelmap'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b9d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Tf records\n",
    "\n",
    "archive_files = os.path.join(paths['images'], 'archive.tar.gz')\n",
    "if os.path.exists(archive_files):\n",
    "    !tar -zxvf \"{archive_files}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a238aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/scripts'...\n",
      "remote: Enumerating objects: 3, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
      "remote: Total 3 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(files['recordscript']):\n",
    "    !git clone https://github.com/nicknochnack/GenerateTFRecord \"{paths['scripts']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "373be7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python3 \"{files['recordscript']}\" -x \"{os.path.join(paths['images'], 'train')}\" -l \"{files['labelmap']}\" -o \"{os.path.join(paths['annotations'], 'train.record')}\" \n",
    "!python3 \"{files['recordscript']}\" -x \"{os.path.join(paths['images'], 'test')}\" -l \"{files['labelmap']}\" -o \"{os.path.join(paths['annotations'], 'test.record')}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7336e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying Model Config to Training Folder\n",
    "\n",
    "!cp \"{os.path.join(paths['pretrained_model'], pretrained_model_name, 'pipeline.config')}\" \"{os.path.join(paths['checkpoint'])}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b09dbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:32:06.267879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Update Config For Transfer Learning\n",
    "\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4b2eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(files['pipeline_config'])\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(files['pipeline_config'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4504e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = len(labels)\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths['pretrained_model'], pretrained_model_name, 'checkpoint', 'ckpt-0')\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= files['labelmap']\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths['annotations'], 'train.record')]\n",
    "pipeline_config.eval_input_reader[0].label_map_path = files['labelmap']\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['annotations'], 'test.record')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a4ac645",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(files['pipeline_config'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afb9e6",
   "metadata": {},
   "source": [
    "### training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e757dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"python3 '/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/models/research/object_detection/model_main_tf2.py' --model_dir='/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model' --pipeline_config_path='/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/pipeline.config' --num_train_steps=10\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_script = os.path.join(paths['apimodel'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "command = \"python3 '{}' --model_dir='{}' --pipeline_config_path='{}' --num_train_steps=10\".format(training_script, paths['checkpoint'],files['pipeline_config'])\n",
    "command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ca601f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 uninstall tensorflow -y\n",
    "# !pip3 install tensorflow==2.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "589fc973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 18:02:49.583206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "I1023 18:03:10.515726 4556797440 mirrored_strategy.py:419] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "INFO:tensorflow:Maybe overwriting train_steps: 10\n",
      "I1023 18:03:10.651760 4556797440 config_util.py:552] Maybe overwriting train_steps: 10\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1023 18:03:10.652095 4556797440 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "W1023 18:03:10.797778 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/model_lib_v2.py:563: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "rename to distribute_datasets_from_function\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/train.record']\n",
      "I1023 18:03:10.827039 4556797440 dataset_builder.py:162] Reading unweighted datasets: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/train.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/train.record']\n",
      "I1023 18:03:10.827656 4556797440 dataset_builder.py:79] Reading record datasets for input file: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/train.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1023 18:03:10.827980 4556797440 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1023 18:03:10.828223 4556797440 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W1023 18:03:10.850138 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1023 18:03:10.934170 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1023 18:03:34.312930 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "W1023 18:03:45.521725 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1023 18:03:52.415495 4556797440 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "2023-10-23 18:04:00.972705: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c198f912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:59:19.498606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
      "W1023 17:59:35.120271 4768265728 model_lib_v2.py:1089] Forced number of epochs for all eval validations to be 1.\n",
      "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "I1023 17:59:35.120450 4768265728 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: None\n",
      "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
      "I1023 17:59:35.120529 4768265728 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
      "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
      "I1023 17:59:35.120609 4768265728 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
      "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "W1023 17:59:35.121806 4768265728 model_lib_v2.py:1106] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
      "INFO:tensorflow:Reading unweighted datasets: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/test.record']\n",
      "I1023 17:59:35.265640 4768265728 dataset_builder.py:162] Reading unweighted datasets: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/test.record']\n",
      "INFO:tensorflow:Reading record datasets for input file: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/test.record']\n",
      "I1023 17:59:35.267211 4768265728 dataset_builder.py:79] Reading record datasets for input file: ['/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/annotations/test.record']\n",
      "INFO:tensorflow:Number of filenames to read: 1\n",
      "I1023 17:59:35.267418 4768265728 dataset_builder.py:80] Number of filenames to read: 1\n",
      "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
      "W1023 17:59:35.267565 4768265728 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "W1023 17:59:35.281072 4768265728 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "W1023 17:59:35.343626 4768265728 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map()\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "W1023 17:59:40.182904 4768265728 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1023 17:59:41.460389 4768265728 deprecation.py:364] From /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Waiting for new checkpoint at /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model\n",
      "I1023 17:59:44.598867 4768265728 checkpoint_utils.py:168] Waiting for new checkpoint at /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/models/research/object_detection/model_main_tf2.py\", line 114, in <module>\n",
      "    tf.compat.v1.app.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/platform/app.py\", line 36, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/absl/app.py\", line 308, in run\n",
      "    _run_main(main, args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/absl/app.py\", line 254, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "             ^^^^^^^^^^\n",
      "  File \"/Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/models/research/object_detection/model_main_tf2.py\", line 81, in main\n",
      "    model_lib_v2.eval_continuously(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/object_detection/model_lib_v2.py\", line 1135, in eval_continuously\n",
      "    for latest_checkpoint in tf.train.checkpoints_iterator(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 226, in checkpoints_iterator\n",
      "    new_checkpoint_path = wait_for_new_checkpoint(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 175, in wait_for_new_checkpoint\n",
      "    time.sleep(seconds_to_sleep)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "\n",
    "\n",
    "command = \"python3 '{}' --model_dir='{}' --pipeline_config_path='{}' --checkpoint_dir='{}'\".format(training_script, paths['checkpoint'],files['pipeline_config'], paths['checkpoint'])\n",
    "!{command}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf83ab",
   "metadata": {},
   "source": [
    "### loading Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36713d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ab59e8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error when restoring from checkpoint or SavedModel at /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/training/py_checkpoint_reader.py:92\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepattern\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# issue with throwing python exceptions from C++.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/checkpoint/checkpoint.py:2677\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2677\u001b[0m   status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2678\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/checkpoint/checkpoint.py:2540\u001b[0m, in \u001b[0;36mCheckpoint.read\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2539\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;129;01mor\u001b[39;00m checkpoint_options\u001b[38;5;241m.\u001b[39mCheckpointOptions()\n\u001b[0;32m-> 2540\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_saver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2541\u001b[0m metrics\u001b[38;5;241m.\u001b[39mAddCheckpointReadDuration(\n\u001b[1;32m   2542\u001b[0m     api_label\u001b[38;5;241m=\u001b[39m_CHECKPOINT_V2,\n\u001b[1;32m   2543\u001b[0m     microseconds\u001b[38;5;241m=\u001b[39m_get_duration_microseconds(start_time, time\u001b[38;5;241m.\u001b[39mtime()))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/checkpoint/checkpoint.py:1415\u001b[0m, in \u001b[0;36mTrackableSaver.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1414\u001b[0m   _ASYNC_CHECKPOINT_THREAD\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m-> 1415\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpy_checkpoint_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNewCheckpointReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1416\u001b[0m graph_building \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/training/py_checkpoint_reader.py:96\u001b[0m, in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 96\u001b[0m   \u001b[43merror_translator\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot found in checkpoint\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to find any \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatching files for\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n\u001b[0;32m---> 31\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, error_message)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSliced checkpoints are not supported\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_message \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData type \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupported\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m error_message:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# restoring checkpoint\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv2\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mCheckpoint(model\u001b[38;5;241m=\u001b[39mdetection_model)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mckpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt-5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpect_partial()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mfunction\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_fn\u001b[39m(image):\n\u001b[1;32m     11\u001b[0m     image, shapes \u001b[38;5;241m=\u001b[39m detection_model\u001b[38;5;241m.\u001b[39mpreprocess(image)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/checkpoint/checkpoint.py:2681\u001b[0m, in \u001b[0;36mCheckpoint.restore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()  \u001b[38;5;66;03m# Ensure restore operations have completed.\u001b[39;00m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2681\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors_impl\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[1;32m   2682\u001b[0m       \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2683\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when restoring from checkpoint or SavedModel at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2684\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2685\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease double-check that the path is correct. You may be missing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2686\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe checkpoint suffix (e.g. the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath/to/ckpt-1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2687\u001b[0m \u001b[38;5;66;03m# Create the save counter now so it gets initialized with other variables\u001b[39;00m\n\u001b[1;32m   2688\u001b[0m \u001b[38;5;66;03m# when graph building. Creating it earlier would lead to errors when using,\u001b[39;00m\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;66;03m# say, train.Saver() to save the model before initializing it.\u001b[39;00m\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_create_save_counter()\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error when restoring from checkpoint or SavedModel at /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for /Users/nazmussakib/Documents/Stuffs/Prog Stuffs/Jupyter Notebooks/hand gestures project/workspace/models/my_model/ckpt-5\nPlease double-check that the path is correct. You may be missing the checkpoint suffix (e.g. the '-1' in 'path/to/ckpt-1')."
     ]
    }
   ],
   "source": [
    "# loading pipeline config and building a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['pipeline_config'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# restoring checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['checkpoint'], 'ckpt-5')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56573bea",
   "metadata": {},
   "source": [
    "### detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc4ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(files['labelmap'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81d7d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(paths['images'], 'test', 'bad.cb5356e8-7177-11ee-a570-d0817a9e9b86.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e987608",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(image_path)\n",
    "image_np = np.array(img)\n",
    "\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy()\n",
    "              for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# detection_classes should be ints.\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "label_id_offset = 1\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np_with_detections,\n",
    "            detections['detection_boxes'],\n",
    "            detections['detection_classes']+label_id_offset,\n",
    "            detections['detection_scores'],\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=5,\n",
    "            min_score_thresh=.8,\n",
    "            agnostic_mode=False)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
